{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjYq95cOvlNf"
      },
      "source": [
        "# Installs and imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wb9FhEByvkfB"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install netCDF4;\n",
        "!pip install visualkeras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sRDBqOnu7Z2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py\n",
        "import netCDF4\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import r2_score,mean_squared_error\n",
        "from google.colab import files\n",
        "from keras.callbacks import Callback\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "import tensorflow as tf\n",
        "from keras.layers import Conv2D, MaxPooling2D, concatenate, Flatten, Dense, Dropout, Input\n",
        "from keras.models import Model\n",
        "import os\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8jlqH-JwZeY",
        "outputId": "d41aad9a-9b2c-4b64-c5e1-52e2b097362b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILY6cHZqwbub"
      },
      "source": [
        "# Read CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4VIMtS0we61"
      },
      "outputs": [],
      "source": [
        "df_robot = pd.read_csv('/info_file/robot_info.csv', sep=';')\n",
        "df_nobal = pd.read_csv('/info_file/nobal_info.csv', sep=';')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ugbwbU-wjRf"
      },
      "outputs": [],
      "source": [
        "# Extract numeric part from a column and convert it to numeric type\n",
        "df_nobal['N_level'] = df_nobal['N_level'].str.extract(r'(\\d+)').astype(float)\n",
        "df_robot['N_level'] = df_robot['N_level'].str.extract(r'(\\d+)').astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dC9gh4VMwm2q"
      },
      "outputs": [],
      "source": [
        "dates_list = ['16.05.2022', '28.06.2022', '21.07.2022', '01.08.2022', '01.09.2022', '20.05.2022', '28.07.2022', '05.08.2022', '31.05.2022', '05.08.2022', '30.08.2022']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owU0RyrCwowA",
        "outputId": "aae37fe6-9631-46c6-b891-6c414acab21d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "mean_value_dth = df_nobal['dth'].mean()\n",
        "df_nobal['dth'].fillna(mean_value_dth, inplace=True)\n",
        "df_nobal['dth'].isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uW0QTO05wpgC"
      },
      "outputs": [],
      "source": [
        "# Flatten the list of dates\n",
        "dates_list = ['16.05.2022', '28.06.2022', '21.07.2022', '01.08.2022', '01.09.2022', '20.05.2022', '28.07.2022', '05.08.2022', '31.05.2022', '05.08.2022', '30.08.2022']\n",
        "\n",
        "# Convert the list of dates to datetime type\n",
        "flat_dates_list = [datetime.strptime(date, '%d.%m.%Y') for date in dates_list]\n",
        "flat_dates_list.sort()\n",
        "\n",
        "# Convert the 'dates' column to datetime type\n",
        "df_robot['HD_day'] = pd.to_datetime(df_robot['HD_day'], format='%d.%m.%Y')\n",
        "days_between = []\n",
        "\n",
        "\n",
        "# Iterate over each date in the flat_dates_list and calculate the days between the date and all dates in the DataFrame\n",
        "for date in flat_dates_list:\n",
        "    days = (df_robot['HD_day'] - date).dt.days\n",
        "    days_between.append(days)\n",
        "\n",
        "# Convert the 'dates' column to datetime type\n",
        "df_nobal['HD_day'] = pd.to_datetime(df_nobal['HD_day'], format='%d.%m.%Y')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcFlFA9-wsnr"
      },
      "outputs": [],
      "source": [
        "mean_value_hd = df_nobal['HD_day'].mean()\n",
        "df_nobal['HD_day'].fillna(mean_value_hd, inplace=True)\n",
        "df_nobal['HD_day'].isna().sum()\n",
        "days_between_nobal = []\n",
        "\n",
        "# Iterate over each date in the flat_dates_list and calculate the days between the date and all dates in the DataFrame\n",
        "for date in flat_dates_list:\n",
        "    days = (df_nobal['HD_day'] - date).dt.days\n",
        "    days_between_nobal.append(days)\n",
        "\n",
        "for item in days_between_nobal:\n",
        "  for number in item:\n",
        "    if math.isnan(number):\n",
        "        print(\"NaN found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBXlreF_wxhj",
        "outputId": "9cbc6e54-9956-4097-bd90-f3fec6a45bcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of nobal_n_level: 192\n",
            "Length of nobal_dth: 192\n",
            "Length of robot_n_level: 288\n",
            "Length of robot_dth: 288\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Concatenate 'N_level' and 'dth' arrays from 'df_nobal' and 'df_robot'\n",
        "nobal_n_level = np.concatenate([df_nobal['N_level'].values, df_nobal['N_level'].values, df_nobal['N_level'].values])\n",
        "nobal_dth = np.concatenate([df_nobal['dth'].values, df_nobal['dth'].values, df_nobal['dth'].values])\n",
        "robot_n_level = np.concatenate([df_robot['N_level'].values, df_robot['N_level'].values, df_robot['N_level'].values])\n",
        "robot_dth = np.concatenate([df_robot['dth'].values, df_robot['dth'].values, df_robot['dth'].values])\n",
        "\n",
        "# Get the length of the resulting arrays\n",
        "len_nobal_n_level = len(nobal_n_level)\n",
        "len_nobal_dth = len(nobal_dth)\n",
        "len_robot_n_level = len(robot_n_level)\n",
        "len_robot_dth = len(robot_dth)\n",
        "\n",
        "# Print the lengths\n",
        "print(\"Length of nobal_n_level:\", len_nobal_n_level)\n",
        "print(\"Length of nobal_dth:\", len_nobal_dth)\n",
        "print(\"Length of robot_n_level:\", len_robot_n_level)\n",
        "print(\"Length of robot_dth:\", len_robot_dth)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eV-rSnV8w0Du"
      },
      "source": [
        "# CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ll5mjWF8wybT"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow import keras\n",
        "def create_model(num_filters=128, kernel_size=3, pool_size=2, dropout_rate=0.2):\n",
        "    # Define input shapes\n",
        "    images_input_shape = (256, 52, 5)  # Input shape for 'images'\n",
        "    days_between_input_shape = (1)  # Input shape for 'dth'\n",
        "\n",
        "    # Create input layers\n",
        "    images_input = Input(shape=images_input_shape, name='images_input')\n",
        "    days_between_input = Input(shape=days_between_input_shape, name='days_between_input')\n",
        "  \n",
        "   \n",
        "    # Add convolutional and pooling layers\n",
        "    conv1 = Conv2D(num_filters, kernel_size=(kernel_size, kernel_size), activation='relu', padding='same')(images_input)\n",
        "    conv2 = Conv2D(num_filters, kernel_size=(kernel_size, kernel_size), activation='relu', padding='same')(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "    conv3 = Conv2D(num_filters, kernel_size=(kernel_size, kernel_size), activation='relu', padding='same')(pool1)\n",
        "    conv4 = Conv2D(num_filters, kernel_size=(kernel_size, kernel_size), activation='relu', padding='same')(conv3)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
        "    conv5 = Conv2D(num_filters, kernel_size=(kernel_size, kernel_size), activation='relu', padding='same')(pool2)\n",
        "    conv6 = Conv2D(num_filters, kernel_size=(kernel_size, kernel_size), activation='relu', padding='same')(conv5)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv6)\n",
        "\n",
        "    # Flatten the feature maps\n",
        "    flatten = Flatten()(pool3)\n",
        "\n",
        "    # Concatenate the input layers along the channel axis\n",
        "    concatenated = concatenate([flatten, days_between_input], axis=-1)\n",
        "\n",
        "    # Add fully connected layers\n",
        "    dense1 = Dense(512, activation='relu')(concatenated)\n",
        "    dropout1 = Dropout(dropout_rate)(dense1)\n",
        "\n",
        "    # Define the model with grain yield as output\n",
        "    gy_output = Dense(units=1, name='gy_output')(dropout1)\n",
        "\n",
        "    # Create the model\n",
        "    model = Model(inputs=[images_input, days_between_input], outputs=[gy_output])\n",
        "    opt = keras.optimizers.Adam(learning_rate=0.00001)\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=opt, loss='mse', metrics=['mse'])\n",
        "    #visualkeras.layered_view(model, legend=True).show()\n",
        "    plot_model(model, to_file='cnn_concat.pdf', show_shapes=True)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DflXccQGw6--"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Conv2D, MaxPooling2D, concatenate, Flatten, Dense, Dropout, Input\n",
        "from keras.models import Model\n",
        "\n",
        "# Define a function that returns the CNN model\n",
        "def create_model_n_level(num_filters=128, kernel_size=3, pool_size=2, dropout_rate=0.2, padding='same'):\n",
        "    # Define input shapes\n",
        "    images_input_shape = (256, 52, 5)  # Input shape for 'images'\n",
        "    fertilization_input_shape = (1)  # Input shape for 'fertilization'\n",
        "    days_between_input_shape = (1)  # Input shape for 'dth'\n",
        "\n",
        "    # Create input layers\n",
        "    images_input = Input(shape=images_input_shape, name='images_input')\n",
        "    fertilization_input = Input(shape=fertilization_input_shape, name='fertilization_input')\n",
        "    days_between_input = Input(shape=days_between_input_shape, name='days_between_input')\n",
        "  \n",
        "   \n",
        "    # Add convolutional and pooling layers\n",
        "    conv1 = Conv2D(num_filters, kernel_size=(kernel_size, kernel_size), activation='relu', padding=padding)(images_input)\n",
        "    conv2 = Conv2D(num_filters, kernel_size=(kernel_size, kernel_size), activation='relu', padding='same')(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "    conv3 = Conv2D(num_filters, kernel_size=(kernel_size, kernel_size), activation='relu', padding='same')(pool1)\n",
        "    conv4 = Conv2D(num_filters, kernel_size=(kernel_size, kernel_size), activation='relu', padding='same')(conv3)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
        "    conv5 = Conv2D(num_filters, kernel_size=(kernel_size, kernel_size), activation='relu', padding='same')(pool2)\n",
        "    conv6 = Conv2D(num_filters, kernel_size=(kernel_size, kernel_size), activation='relu', padding='same')(conv5)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv6)\n",
        "\n",
        "    # Flatten the feature maps\n",
        "    flatten = Flatten()(pool3)\n",
        "\n",
        "    # Concatenate the input layers along the channel axis\n",
        "    concatenated = concatenate([flatten, fertilization_input, days_between_input], axis=-1)\n",
        "\n",
        "    # Add fully connected layers\n",
        "    dense1 = Dense(512, activation='relu')(concatenated)\n",
        "    dropout1 = Dropout(dropout_rate)(dense1)\n",
        "\n",
        "    # Define the model with grain yield as output\n",
        "    gy_output = Dense(units=1, name='gy_output')(dropout1)\n",
        "\n",
        "    # Create the model\n",
        "    model = Model(inputs=[images_input, fertilization_input, days_between_input], outputs=[gy_output])\n",
        "\n",
        "    # Compile the model\n",
        "    opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "    model.compile(optimizer=opt, loss='mse', metrics=['mse'])\n",
        "    #plot_model(model, to_file='cnn_concat_n_level.pdf', show_shapes=True)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv2NsqYpxGi0"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AMiJrtRxAXw"
      },
      "outputs": [],
      "source": [
        "epochs = 1\n",
        "num_filters=128\n",
        "kernel_size=2\n",
        "pool_size=3\n",
        "dropout_rate=0.2\n",
        "batch_size = 16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKDbuEuKxKPi"
      },
      "source": [
        "# Function for visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0r9k_3mxNnd"
      },
      "outputs": [],
      "source": [
        "def residualplot(y_pred, y_true, dsh, figurename):\n",
        "    # Set fontsize\n",
        "    xlim_max = 8\n",
        "    xlim_min = 3\n",
        "    ylim_max = 5\n",
        "    ylim_min = -5\n",
        "\n",
        "    # Calculate residuals and their z-scores\n",
        "    residuals = y_true - y_pred\n",
        "    residual_mean = np.mean(residuals)\n",
        "    residual_std = np.std(residuals)\n",
        "    z_scores = (residuals - residual_mean) / residual_std\n",
        "\n",
        "    # Create residual plots using Seaborn\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.residplot(x=y_pred, y=z_scores, scatter_kws={'alpha': 0.5}, hue=dsh)\n",
        "    plt.xlabel('Predicted Values', fontsize=18)\n",
        "    plt.ylabel('Standardized Residuals', fontsize=18)\n",
        "    plt.title(f'Residual Plot (Model {figurename})',fontsize=18)\n",
        "    plt.xticks(fontsize=18)\n",
        "    plt.yticks(fontsize=18)\n",
        "    plt.ylim(ylim_min, ylim_max)  # Set y-axis range\n",
        "    plt.xlim(xlim_min, xlim_max)  # Set x-axis range\n",
        "    plt.tight_layout()  # Add\n",
        "    plt.savefig(f'resplot_Model_{figurename}.pdf', bbox_inches='tight')\n",
        "    files.download(f'resplot_Model_{figurename}.pdf')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08HtSFUWxUYR"
      },
      "outputs": [],
      "source": [
        "def scatterplot(y_pred, y_true, r2, mse, mbd, dsh, figurename):\n",
        "    # Create scatter plot with Seaborn for nobal\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.rcParams['legend.fontsize'] = 18\n",
        "    sns.scatterplot(x=y_true, y=y_pred, color='#8bad84', s=70, hue=dsh)\n",
        "    plt.xlabel('Measured grain yield (t/ha)', fontsize=18)\n",
        "    plt.ylabel('Predicted grain yield (t/ha)', fontsize=18)\n",
        "    plt.title(f'Actual vs. Predicted Values (Model {figurename})', fontsize=18)\n",
        "    sns.lineplot(x=[4.2, 7.8], y=[4.2, 7.8])\n",
        "    plt.xticks(fontsize=18)\n",
        "    plt.yticks(fontsize=18)\n",
        "    plt.ylim(4, 8)  # Set y-axis range\n",
        "    plt.xlim(4, 8)  # Set x-axis range\n",
        "    plt.annotate('MSE: {:.2f}'.format(mse), (4.2, 7.8), ha='left', va='top', fontsize=16)  # Add MBD as text annotation\n",
        "    plt.annotate(f'R-squared: {r2:.3f}', (4.2, 7.6), ha='left', va='top', fontsize=16)  # Add R-squared as text annotation\n",
        "    #plt.annotate(f'MBE: {mbd:.3f}', (4.2, 7.4), ha='left', va='top', fontsize=16)  # Add R-squared as text annotation\n",
        "    plt.legend(loc='upper left', bbox_to_anchor=(1.02, 1), borderaxespad=0, fontsize=16)  # Add legend outside the plot\n",
        "    plt.savefig(f'scatter_Model_{figurename}.pdf', bbox_inches='tight')\n",
        "    files.download(f'scatter_Model_{figurename}.pdf')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0mic536xXQJ"
      },
      "outputs": [],
      "source": [
        "def normalize(X):\n",
        "  X_min = np.min(X)\n",
        "  X_max = np.max(X)\n",
        "  X_norm = (X - X_min) / (X_max - X_min)\n",
        "  return X_norm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VggnKUOzxcK-"
      },
      "source": [
        "# Read all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJfPet5KxaoA"
      },
      "outputs": [],
      "source": [
        "# Initialize empty lists to store X and y\n",
        "X_all_dth_date = []\n",
        "y_all_dth_date = []\n",
        "dates = []\n",
        "\n",
        "# Loop through the files\n",
        "for i in range(1,8):\n",
        "    # Read data from h5 file\n",
        "    with h5py.File(f'/datasets/datetime_robot_{i}.h5', 'r') as hf:\n",
        "        # Append data to X_list and y_list\n",
        "        X_all_dth_date.extend(hf['X'][:])\n",
        "        y_all_dth_date.extend(hf['y'][:])\n",
        "\n",
        "    # Extend date_captured list with three days between\n",
        "    dates.extend([days_between[i], days_between[i], days_between[i]])\n",
        "\n",
        "# Convert X_list_dth_date and y_dth_date to numpy arrays\n",
        "X = np.array(X_all_dth_date)\n",
        "y_all_dth_date = np.array(y_all_dth_date)\n",
        "X = np.transpose(X_all_dth_date, (0,2,3,1))\n",
        "dates = np.array(dates).flatten()\n",
        "# Apply min-max normalization to the X data\n",
        "\n",
        "X_norm_all = normalize(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H31jGU1Nxhoy"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test_all, days_train, days_test = train_test_split(X_norm_all, y_all_dth_date, dates, test_size=0.2, random_state=42, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8d4W8phxs-B",
        "outputId": "71181c07-812b-46f2-a4bf-2610712e4394"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "81/81 [==============================] - 17s 72ms/step - loss: 16.5894 - mse: 16.5894 - val_loss: 2.6458 - val_mse: 2.6458\n"
          ]
        }
      ],
      "source": [
        "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=50)\n",
        "model = create_model(num_filters=num_filters, kernel_size=kernel_size, pool_size=pool_size, dropout_rate=dropout_rate)\n",
        "model_history = model.fit([X_train, days_train], y_train,\n",
        "                    batch_size=batch_size, \n",
        "                    epochs=epochs, \n",
        "                    validation_split=0.2,\n",
        "                    callbacks=[early_stopping_callback], \n",
        "                    shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C16D6JwZyHWV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fab0d19-5349-4a89-ecd0-21c90fbd2710"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26/26 [==============================] - 1s 19ms/step\n"
          ]
        }
      ],
      "source": [
        "pred_all = model.predict([X_test, days_test], batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read one"
      ],
      "metadata": {
        "id": "QPWgkg0mtVYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mse_one_scores = []\n",
        "datetime = []\n",
        "date = 8\n",
        "\n",
        "# Create an instance of the custom callback\n",
        "with h5py.File(f'/datasets/datetime_robot_{date}.h5', 'r') as hf:\n",
        "    print(hf.keys())\n",
        "    X = hf['X'][:]\n",
        "    y = hf['y'][:]\n",
        "\n",
        "\n",
        "\n",
        "datetime.extend([days_between[date], days_between[date], days_between[date]])\n",
        "datetime = np.array(datetime).flatten()\n",
        "X = np.array(X)               \n",
        "X = np.transpose(X, (0,2,3,1))\n",
        "y = np.array(y)\n",
        "\n",
        "\n",
        "X_norm_one = normalize(X)"
      ],
      "metadata": {
        "id": "58R9RLyytUkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=100)\n",
        "X_train, X_test, y_train, y_test_one, datetime_train, datetime_test = train_test_split(X_norm_one, y, datetime, test_size=0.2, random_state=42, shuffle=True)"
      ],
      "metadata": {
        "id": "EVplstv7tc4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the combined data into training and testing sets\n",
        "model = create_model(num_filters=num_filters, kernel_size=kernel_size, pool_size=pool_size, dropout_rate=dropout_rate)\n",
        "model_history = model.fit([X_train, datetime_train], y_train,\n",
        "                  batch_size=batch_size, \n",
        "                  epochs=epochs, \n",
        "                  validation_split=0.2,\n",
        "                  callbacks=[early_stopping_callback], \n",
        "                  shuffle=True)"
      ],
      "metadata": {
        "id": "p6vfb8zdtfJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_one = model.predict([X_test, datetime_test], batch_size=batch_size)"
      ],
      "metadata": {
        "id": "d_4GDHfNtuqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataframe to store the predicted and true values\n",
        "df = pd.DataFrame({'predicted': pred_one.reshape(-1), 'true': y_test_one.reshape(-1),  'DSH': datetime_test})\n",
        "\n",
        "# Write the dataframe to a CSV file\n",
        "df.to_csv('/csv_files/predicted_vs_true_SingleDateTimeRobot.csv', index=False)"
      ],
      "metadata": {
        "id": "zNR8xmrBt_Q2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nobal in training, Robot in test"
      ],
      "metadata": {
        "id": "2G89reXqt8VB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize empty lists to store X and y\n",
        "X_list_robot = []\n",
        "y_list_robot = []\n",
        "date_captured_robot = []\n",
        "\n",
        "X_list_nobal = []\n",
        "y_list_nobal = []\n",
        "date_captured_nobal = []\n",
        "\n",
        "# Loop through the files\n",
        "for i in range(1,9):\n",
        "    # Read data from h5 file\n",
        "    with h5py.File(f'/datasets/datetime_robot_{i}.h5', 'r') as robot:\n",
        "        # Append data to X_list and y_list\n",
        "        X_list_robot.extend(robot['X'][:])\n",
        "        y_list_robot.extend(robot['y'][:])\n",
        "    with h5py.File(f'/datasets/datetime_nobal_{i}.h5', 'r') as nobal:\n",
        "      X_list_nobal.extend(nobal['X'][:])\n",
        "      y_list_nobal.extend(nobal['y'][:])\n",
        "\n",
        "    date_captured_robot.extend([days_between[i], days_between[i], days_between[i]])\n",
        "    date_captured_nobal.extend([days_between_nobal[i], days_between_nobal[i], days_between_nobal[i]])\n",
        "\n",
        "# Convert X_list_dth_date and y_dth_date to numpy arrays\n",
        "X_list_robot = np.array(X_list_robot)\n",
        "X_list_robot = np.transpose(X_list_robot, (0,2,3,1))\n",
        "y_list_robot = np.array(y_list_robot)\n",
        "date_captured_robot = np.array(date_captured_robot).flatten()\n",
        "\n",
        "X_list_nobal = np.array(X_list_nobal)\n",
        "X_list_nobal = np.transpose(X_list_nobal, (0,2,3,1))\n",
        "y_list_nobal = np.array(y_list_nobal).squeeze()\n",
        "date_captured_nobal = np.array(date_captured_nobal).flatten()\n",
        "\n",
        "\n",
        "X_norm_robot = normalize(X_list_robot)\n",
        "X_norm_nobal = normalize(X_list_nobal)"
      ],
      "metadata": {
        "id": "RDWxxjkIuCck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10)\n",
        "model = create_model(num_filters=num_filters, kernel_size=kernel_size, pool_size=pool_size, dropout_rate=dropout_rate)\n",
        "model_history = model.fit([X_norm_nobal, date_captured_nobal], y_list_nobal,\n",
        "                  batch_size=batch_size, \n",
        "                  epochs=epochs, \n",
        "                  verbose=2,\n",
        "                  callbacks=[early_stopping_callback], \n",
        "                  validation_split=0.2, \n",
        "                  shuffle=True)"
      ],
      "metadata": {
        "id": "CJHPBF3IuH1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_only_nobal = model.predict([X_norm_robot ,date_captured_robot], batch_size=batch_size)"
      ],
      "metadata": {
        "id": "P4VBIHnducYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataframe to store the predicted and true values\n",
        "df = pd.DataFrame({'predicted': pred_only_nobal.reshape(-1), 'true': y_list_robot.reshape(-1), 'DSH': date_captured_robot})\n",
        "\n",
        "# Write the dataframe to a CSV file\n",
        "df.to_csv('/csv_files/predicted_vs_true_SeparatedDataTestRobot.csv', index=False)"
      ],
      "metadata": {
        "id": "aEQqOZS7un1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mixed Data"
      ],
      "metadata": {
        "id": "8zZfwFA9uil1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize empty lists to store X and y\n",
        "X_list_robot = []\n",
        "y_list_robot = []\n",
        "date_captured_robot = []\n",
        "robot_n_level_date = []\n",
        "\n",
        "X_list_nobal = []\n",
        "y_list_nobal = []\n",
        "date_captured_nobal = []\n",
        "nobal_n_level_date = []\n",
        "\n",
        "# Loop through the files\n",
        "for i in range(1,9):\n",
        "    # Read data from h5 file\n",
        "    with h5py.File(f'/datasets/datetime_robot_{i}.h5', 'r') as robot:\n",
        "        # Append data to X_list and y_list\n",
        "        X_list_robot.extend(robot['X'][:])\n",
        "        y_list_robot.extend(robot['y'][:])\n",
        "        robot_n_level_date.extend(robot_n_level)\n",
        "    with h5py.File(f'/datasets/nobal_datetime_{i}.h5', 'r') as nobal:\n",
        "      X_list_nobal.extend(nobal['X'][:])\n",
        "      y_list_nobal.extend(nobal['y'][:])\n",
        "\n",
        "    date_captured_robot.extend([days_between[i], days_between[i], days_between[i]])\n",
        "    date_captured_nobal.extend([days_between_nobal[i], days_between_nobal[i], days_between_nobal[i]])\n",
        "    \n",
        "    \n",
        "\n",
        "# Convert X_list_dth_date and y_dth_date to numpy arrays\n",
        "X_list_robot = np.array(X_list_robot)\n",
        "X_list_robot = np.transpose(X_list_robot, (0,2,3,1))\n",
        "y_list_robot = np.array(y_list_robot)\n",
        "date_captured_robot = np.array(date_captured_robot).flatten()\n",
        "\n",
        "X_list_nobal = np.array(X_list_nobal)\n",
        "X_list_nobal = np.transpose(X_list_nobal, (0,2,3,1))\n",
        "y_list_nobal = np.array(y_list_nobal).squeeze()\n",
        "date_captured_nobal = np.array(date_captured_nobal).flatten()\n",
        "\n",
        "X_both = np.concatenate((X_list_nobal, X_list_robot))\n",
        "y_both = np.concatenate((y_list_nobal, y_list_robot))\n",
        "date_captured_both = np.concatenate((date_captured_nobal, date_captured_robot))\n",
        "\n",
        "\n",
        "X_norm_both = normalize(X_both)"
      ],
      "metadata": {
        "id": "T4H0dxHQulGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=100)\n",
        "X_train, X_test_both, y_train, y_test_nobal, date_captured_both_train, date_captured_both_test = train_test_split(X_norm_both, y_both, date_captured_both, test_size=0.2, random_state=42, shuffle=True)\n",
        "\n",
        "model = create_model(num_filters=num_filters, kernel_size=kernel_size, pool_size=(2, 2, 1), dropout_rate=dropout_rate)\n",
        "model_history = model.fit([X_train, date_captured_both_train], y_train,\n",
        "                batch_size=batch_size, \n",
        "                epochs=epochs, \n",
        "                callbacks=[early_stopping_callback], \n",
        "                validation_split=0.2, \n",
        "                shuffle=True)\n",
        "\n",
        "pred_nobal = model.predict([X_test_both, date_captured_both_test], batch_size=batch_size)"
      ],
      "metadata": {
        "id": "kO8puZTLusrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataframe to store the predicted and true values\n",
        "df = pd.DataFrame({'predicted': pred_nobal.reshape(-1), 'true': y_test_nobal.reshape(-1), 'DSH': date_captured_both_test})\n",
        "\n",
        "# Write the dataframe to a CSV file\n",
        "df.to_csv('csv_files/predicted_vs_true_MixedData.csv', index=False)"
      ],
      "metadata": {
        "id": "n7J1eevdzgLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBy-dzLzvCty"
      },
      "source": [
        "# DTH and Fertilization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zN2E_NZVvF9r"
      },
      "outputs": [],
      "source": [
        "# Initialize empty lists to store X and y\n",
        "X_list_dth_date = []\n",
        "y_dth_date = []\n",
        "date_captured = []\n",
        "#robot_dth_date = []\n",
        "robot_n_level_date = []\n",
        "# Loop through the files\n",
        "for i in range(1,9):\n",
        "    # Read data from h5 file\n",
        "    with h5py.File(f'/datasets/datetime_robot_{i}.h5', 'r') as hf:\n",
        "        # Append data to X_list_dth_date and y_dth_date\n",
        "        X_list_dth_date.extend(hf['X'][:])\n",
        "        y_dth_date.extend(hf['y'][:])\n",
        "        \n",
        "    # Extend date_captured list with three days between\n",
        "    date_captured.extend([days_between[i], days_between[i], days_between[i]])\n",
        "    #robot_dth_date.extend(robot_dth)\n",
        "    robot_n_level_date.extend(robot_n_level)\n",
        "\n",
        "\n",
        "# Convert X_list_dth_date and y_dth_date to numpy arrays\n",
        "X_dth_date = np.array(X_list_dth_date)\n",
        "y_dth_date = np.array(y_dth_date)\n",
        "X_dth_date = np.transpose(X_dth_date, (0,2,3,1))\n",
        "\n",
        "X_norm_dth = normalize(X_dth_date)\n",
        "\n",
        "data_captured_dth = np.array(date_captured).flatten().reshape(-1, 1)\n",
        "#robot_dth_date = np.array(robot_dth_date).reshape(-1, 1)\n",
        "robot_n_level_date = np.asarray(robot_n_level_date).reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_axxgC2EveUK"
      },
      "outputs": [],
      "source": [
        "# Train test split\n",
        "X_dth_date_train, X_dth_date_test, y_dth_date_train, y_dth_date_test, data_captured_dth_train, data_captured_dth_test, robot_n_level_date_train, robot_n_level_date_test = train_test_split(X_norm_dth, y_dth_date, data_captured_dth, \n",
        "                                           robot_n_level_date,  test_size=0.2, random_state=42, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6UprN83vnup"
      },
      "outputs": [],
      "source": [
        "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wJAH7C3vsH6",
        "outputId": "4b473c95-6e96-4dd0-e7ad-0d80305ab15e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "93/93 [==============================] - 9s 70ms/step - loss: 3.9174 - mse: 3.9174 - val_loss: 0.8298 - val_mse: 0.8298\n"
          ]
        }
      ],
      "source": [
        "model = create_model_n_level(num_filters=num_filters, kernel_size=kernel_size, pool_size=2, dropout_rate=dropout_rate)\n",
        "model_history = model.fit([X_dth_date_train, robot_n_level_date_train, data_captured_dth_train], y_dth_date_train, \n",
        "                        batch_size=batch_size, \n",
        "                        epochs=epochs, \n",
        "                        callbacks=[early_stopping_callback], \n",
        "                        validation_split=0.2, \n",
        "                        shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BmHxhFRwULX",
        "outputId": "da81888d-49f3-4f0c-9f2c-77fff98bb6c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29/29 [==============================] - 1s 26ms/step\n"
          ]
        }
      ],
      "source": [
        "pred_dth_datetime = model.predict([X_dth_date_test, robot_n_level_date_test, data_captured_dth_test], batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XY9Ky36bwWiX"
      },
      "outputs": [],
      "source": [
        "# Create a dataframe to store the predicted and true values\n",
        "df = pd.DataFrame({'predicted': pred_dth_datetime.reshape(-1), 'true': y_dth_date_test.reshape(-1), 'DSH': data_captured_dth_test.squeeze()})\n",
        "\n",
        "# Write the dataframe to a CSV file\n",
        "df.to_csv('/csv_files/predicted_vs_true_DateTimeConcatenationRobot_new.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHvSITptwciZ"
      },
      "source": [
        "# Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThaYbp9Awbpd"
      },
      "outputs": [],
      "source": [
        "# assign directory\n",
        "directory = '/csv_files'\n",
        "\n",
        "# iterate over files in that directory\n",
        "for filename in os.listdir(directory):\n",
        "    f = os.path.join(directory, filename)\n",
        "    # checking if it is a file\n",
        "    if os.path.isfile(f):\n",
        "        # extract the last name of the file without extension\n",
        "        last_name = os.path.splitext(os.path.basename(filename))[0]\n",
        "        # split the last name by '_'\n",
        "        name_parts = last_name.split('_')\n",
        "        # get the last part of the name\n",
        "        plot_name = name_parts[-2]\n",
        "        df = pd.read_csv(f)\n",
        "        y_true = df['true']\n",
        "        y_predicted = df['predicted']\n",
        "        dsh = df['DSH']\n",
        "\n",
        "        residuals_one = y_true - y_predicted\n",
        "        r2 = r2_score(y_true, y_predicted)\n",
        "        mse = mean_squared_error(y_true, y_predicted)\n",
        "        mbd = np.mean(y_predicted - y_true)\n",
        "\n",
        "        scatterplot(y_predicted, y_true, r2, mse, mbd, dsh,plot_name)\n",
        "        residualplot(y_predicted, y_true, plot_name)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "ZjYq95cOvlNf"
      ]
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}