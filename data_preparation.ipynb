{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tufPv7G7aasF"
      },
      "source": [
        "# Installs and imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ozq7WzspoNMA"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install rioxarray;\n",
        "!pip install geopandas;\n",
        "!pip install spectral;\n",
        "!pip install netCDF4;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "XX68lScvoSjR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import spectral.io.envi as envi\n",
        "from skimage import io\n",
        "import h5py\n",
        "import netCDF4 as nc\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import xarray as xr\n",
        "import rioxarray as rio\n",
        "from pathlib import Path\n",
        "import rasterio as rio\n",
        "from rasterio.mask import mask\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import mapping\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhYc1YLmoVVa",
        "outputId": "e0456e34-032f-498e-98c1-335a32ae8a28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxZXJZHw79F1"
      },
      "source": [
        "# Ortofoto robot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIvG2i2woKYl"
      },
      "outputs": [],
      "source": [
        "plot_shapes = gpd.read_file('/content/drive/MyDrive/Master/ground_truth/robot-2022-with-grain-yield.geojson')\n",
        "ds = xr.open_dataset('/content/drive/MyDrive/Master/ground_truth/robot-2022-P4M-50m.nc', decode_coords='all')\n",
        "plot_shapes = plot_shapes.loc[(plot_shapes[\"plot_id\"] < 9000) & (plot_shapes[\"plot_id\"] != 0)]\n",
        "\n",
        "# convert GeoSeries to GeoDataFrame\n",
        "plot_shapes = gpd.GeoDataFrame(plot_shapes)\n",
        "\n",
        "\n",
        "clipped_polygons = []\n",
        "for polygon in plot_shapes[\"geometry\"]:\n",
        "  clipped_ds = ds.rio.clip([mapping(polygon)])\n",
        "  clipped_polygons.append(clipped_ds.to_array())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Wk4AABPaLb4t"
      },
      "outputs": [],
      "source": [
        "# Desired size of the images\n",
        "new_shape = (256,256)\n",
        "\n",
        "# Initialize list to store resized and rotated images\n",
        "resized_and_rotated_polygons = []\n",
        "\n",
        "# Loop through each image in the list\n",
        "for polygon in clipped_polygons:\n",
        "    # Reshape image using the new shape\n",
        "    resized_polygon = np.zeros((polygon.shape[0], polygon.shape[1], new_shape[1], new_shape[0]))\n",
        "    for i in range(polygon.shape[0]):\n",
        "        for j in range(polygon.shape[1]):\n",
        "            # Convert DataArray to numpy array\n",
        "            polygon_array = polygon[i, j].values\n",
        "            # Resize image\n",
        "            resized_image = np.array(Image.fromarray(polygon_array).resize(new_shape))\n",
        "            # Rotate image\n",
        "            rotated_image = Image.fromarray(resized_image).rotate(39.6, resample=Image.BICUBIC)\n",
        "            # Convert rotated image back to numpy array\n",
        "            rotated_image_array = np.array(rotated_image)\n",
        "            resized_polygon[i, j] = rotated_image_array\n",
        "    # Add resized and rotated image to list\n",
        "    resized_and_rotated_polygons.append(resized_polygon)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "hYf6jEWePyVL"
      },
      "outputs": [],
      "source": [
        "final_plots = []\n",
        "for image in resized_and_rotated_polygons:\n",
        "  final_plots.append(image[:,:,:,102:154])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mgc8V6_xR6D"
      },
      "source": [
        "# Ortofoto nobal nue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwBLpWohxQx-"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import xarray as xr\n",
        "import rioxarray as rio\n",
        "from shapely.wkt import loads\n",
        "\n",
        "plot_shapes_nobal = gpd.read_file('/content/drive/MyDrive/Master/ground_truth/nobal_nue_gy.geojson')\n",
        "ds_nobal = xr.open_dataset('/content/drive/MyDrive/Master/ground_truth/nobalnue-2022-P4M-20m.nc', decode_coords='all')\n",
        "plot_shapes_nobal = plot_shapes_nobal.loc[(plot_shapes_nobal[\"plot_id\"] < 9000) & (plot_shapes_nobal[\"plot_id\"] != 0)]\n",
        "\n",
        "# convert GeoSeries to GeoDataFrame\n",
        "plot_shapes_nobal = gpd.GeoDataFrame(plot_shapes_nobal)\n",
        "\n",
        "from shapely.geometry import mapping\n",
        "\n",
        "clipped_polygons_nobalnue = []\n",
        "for polygon in plot_shapes_nobal[\"geometry\"]:\n",
        "  clipped_ds = ds_nobal.rio.clip([mapping(polygon)])\n",
        "  clipped_polygons_nobalnue.append(clipped_ds.to_array())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHbZmSD9xd0h"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Desired size of the images\n",
        "new_shape = (256,256)\n",
        "\n",
        "# Initialize list to store resized and rotated images\n",
        "resized_and_rotated_polygons_nobal = []\n",
        "\n",
        "# Loop through each image in the list\n",
        "for polygon in clipped_polygons_nobalnue:\n",
        "    # Reshape image using the new shape\n",
        "    resized_polygon = np.zeros((polygon.shape[0], polygon.shape[1], new_shape[1], new_shape[0]))\n",
        "    for i in range(polygon.shape[0]):\n",
        "        for j in range(polygon.shape[1]):\n",
        "            # Convert DataArray to numpy array\n",
        "            polygon_array = polygon[i, j].values\n",
        "            # Resize image\n",
        "            resized_image = np.array(Image.fromarray(polygon_array).resize(new_shape))\n",
        "            # Rotate image\n",
        "            rotated_image = Image.fromarray(resized_image).rotate(39.6, resample=Image.BICUBIC)\n",
        "            # Convert rotated image back to numpy array\n",
        "            rotated_image_array = np.array(rotated_image)\n",
        "            resized_polygon[i, j] = rotated_image_array\n",
        "            #print(resized_polygon.shape)\n",
        "    # Add resized and rotated image to list\n",
        "    resized_and_rotated_polygons_nobal.append(resized_polygon)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXqzmGC8xgQq"
      },
      "outputs": [],
      "source": [
        "final_plots_nobal = []\n",
        "for image in resized_and_rotated_polygons_nobal:\n",
        "  final_plots_nobal.append(image[:,:,:,102:154])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFAShucIOHtT"
      },
      "source": [
        "# Ground truth both robot and nobal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Thnn_OVzOLIm"
      },
      "outputs": [],
      "source": [
        "df_robot = pd.read_csv('/content/drive/MyDrive/Master/info_fil/robot_info.csv', sep=';')\n",
        "df_nobal = pd.read_csv('/content/drive/MyDrive/Master/info_fil/nobal_info.csv', sep=';')\n",
        "# Replace comma with period in 'num_str' column and convert to float type\n",
        "df_robot['GY'] = df_robot['GY'].str.replace(',', '.').astype(float)\n",
        "df_nobal['GY'] = df_nobal['GY'].str.replace(',', '.').astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zVzjhPNOW_D"
      },
      "outputs": [],
      "source": [
        "mean_value_gy = round(df_robot['GY'].mean(), 2)\n",
        "df_robot['GY'].fillna(mean_value_gy, inplace=True)\n",
        "df_robot['GY'].isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcWkxZK3ocwV"
      },
      "outputs": [],
      "source": [
        "mean_value_gy_nobal = round(df_nobal['GY'].mean())\n",
        "df_nobal['GY'].fillna(mean_value_gy_nobal, inplace=True)\n",
        "df_nobal['GY'].isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ol6Zl2SJoqTm"
      },
      "outputs": [],
      "source": [
        "target_nobal = np.array(df_nobal['GY'])\n",
        "target_robot = np.array(df_robot['GY'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_J9wNitybTb"
      },
      "source": [
        "# Data Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmI-Zs7PzDrc"
      },
      "source": [
        "## Nobal nue dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3uIH7LIRwlD"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "date_images_nobal = defaultdict(list)\n",
        "\n",
        "for image in final_plots_nobal:\n",
        "    #print(image)\n",
        "    for date in range(image.shape[1]):\n",
        "      date_images_nobal[f'nobal_{date}'].append(image[1:,date,:, :])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzGtyuuIzSLg"
      },
      "outputs": [],
      "source": [
        "X_mirrored_v_nobal = {}\n",
        "for date, lst_of_images in date_images_nobal.items(): \n",
        "  X_mirrored_v_nobal[f'{date}'] = []\n",
        "  for image in lst_of_images:\n",
        "    mirrored_image = image[:,::-1]\n",
        "    X_mirrored_v_nobal[f'{date}'].append(mirrored_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEFMPryszkPi"
      },
      "outputs": [],
      "source": [
        "X_mirrored_h_nobal = {}\n",
        "for date, lst_of_images in date_images_nobal.items(): \n",
        "  X_mirrored_h_nobal[f'{date}'] = []\n",
        "  for image in lst_of_images:\n",
        "    mirrored_image = image[::-1,:]\n",
        "    X_mirrored_h_nobal[f'{date}'].append(mirrored_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3U6Or_VBzMvt"
      },
      "source": [
        "## Robot dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eesvX9gZzPNX"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "date_images = defaultdict(list)\n",
        "\n",
        "for image in final_plots:\n",
        "    #print(image)\n",
        "    for date in range(image.shape[1]):\n",
        "      date_images[f'robot_{date}'].append(image[1:,date,:, :])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFjzFZkwyZeK"
      },
      "outputs": [],
      "source": [
        "X_mirrored_v = {}\n",
        "for date, lst_of_images in date_images.items(): \n",
        "  X_mirrored_v[f'{date}'] = []\n",
        "  for image in lst_of_images:\n",
        "    mirrored_image = image[:,::-1]\n",
        "    X_mirrored_v[f'{date}'].append(mirrored_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zv6xXVcvVc-X"
      },
      "outputs": [],
      "source": [
        "X_mirrored_h = {}\n",
        "for date, lst_of_images in date_images.items(): \n",
        "  X_mirrored_h[f'{date}'] = []\n",
        "  for image in lst_of_images:\n",
        "    mirrored_image = image[::-1,:]\n",
        "    X_mirrored_h[f'{date}'].append(mirrored_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3qTnfpLzoLz"
      },
      "source": [
        "## The final dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TT4qwEc36zK"
      },
      "outputs": [],
      "source": [
        "#ROBOT\n",
        "date_images_final_robot = {}\n",
        "for date, images in date_images.items():\n",
        "    date_images_final_robot[f'{date}'] = images\n",
        "for date, images in X_mirrored_v.items():\n",
        "    if date in date_images_final_robot:\n",
        "        date_images_final_robot[f'{date}'].extend(images)\n",
        "    else:\n",
        "        date_images_final_robot[f'{date}'] = images\n",
        "for date, images in X_mirrored_h.items():\n",
        "    if date in date_images_final_robot:\n",
        "        date_images_final_robot[f'{date}'].extend(images)\n",
        "    else:\n",
        "        date_images_final_robot[f'{date}'] = images\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXimTs_yFzJZ"
      },
      "outputs": [],
      "source": [
        "#NOBAL\n",
        "date_images_final_nobal = {}\n",
        "for date, images in date_images_nobal.items():\n",
        "    date_images_final_nobal[f'{date}'] = images\n",
        "for date, images in X_mirrored_v_nobal.items():\n",
        "    if date in date_images_final_nobal:\n",
        "        date_images_final_nobal[f'{date}'].extend(images)\n",
        "    else:\n",
        "        date_images_final_nobal[f'{date}'] = images\n",
        "for date, images in X_mirrored_h_nobal.items():\n",
        "    if date in date_images_final_nobal:\n",
        "        date_images_final_nobal[f'{date}'].extend(images)\n",
        "    else:\n",
        "        date_images_final_nobal[f'{date}'] = images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_t7sLb7qtSt"
      },
      "source": [
        "# Write to HDF5 file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8zIluwGqy82"
      },
      "outputs": [],
      "source": [
        "y_robot = target_robot + target_robot + target_robot\n",
        "y_nobal = target_nobal + target_nobal + target_nobal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKpaRjWBFkIP"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "\n",
        "for date, image in date_images_final_robot.items():\n",
        "  X = image\n",
        "  #X_test = image[200:]\n",
        "\n",
        "  y = target_robot\n",
        "  #y_test = final_lst_target[200:] \n",
        "  # Define the HDF5 file name and location\n",
        "  hdf5_file = f\"/datasets/datetime_{date}.h5\"\n",
        "\n",
        "  # Define the number of images\n",
        "  num_images = len(X)\n",
        "  # Create the HDF5 file\n",
        "  with h5py.File(hdf5_file, \"w\") as hf:\n",
        "      # Create the datasets for the images and targets\n",
        "      hf.create_dataset(\"X\", data=X)\n",
        "      #hf.create_dataset(\"X_test\", data=X_test)\n",
        "      hf.create_dataset(\"y\", data=y)\n",
        "     # hf.create_dataset(\"y_test\", data=y_test)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNJ0dTbhbXCO"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "\n",
        "for date, image in date_images_final_nobal.items():\n",
        "  X = image\n",
        "  y = target_nobal\n",
        "\n",
        "  # Define the HDF5 file name and location\n",
        "  hdf5_file = f\"/datasets/datetime_{date}.h5\"\n",
        "\n",
        "  # Define the number of images\n",
        "  num_images = len(X)\n",
        "\n",
        "  # Create the HDF5 file\n",
        "  with h5py.File(hdf5_file, \"w\") as hf:\n",
        "      # Create the datasets for the images and targets\n",
        "      hf.create_dataset(\"X\", data=X)\n",
        "      hf.create_dataset(\"y\", data=y)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "tufPv7G7aasF",
        "SxZXJZHw79F1",
        "DFAShucIOHtT",
        "7mgc8V6_xR6D",
        "GXnA95sbvhX5",
        "RFrw_WGH8KuN",
        "L1tbHudc88Dt",
        "BbomCfO_72uK",
        "KOWIKJ3Dcykt",
        "CNFf_8YL_6u7",
        "qmI-Zs7PzDrc",
        "3U6Or_VBzMvt",
        "a3qTnfpLzoLz",
        "hZ9slNhobYqS",
        "r_t7sLb7qtSt"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}